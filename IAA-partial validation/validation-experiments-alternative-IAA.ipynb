{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.metrics import agreement\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "from nltk.metrics import masi_distance, jaccard_distance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.spatial import distance\n",
    "from itertools import permutations, combinations\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The string emotion labels have previously been translated using the dictionary below:\n",
    "translation_dict = {\n",
    "    'Angst': (1,0),\n",
    "    'Aanvaarding': (0.707,0.707),\n",
    "    'Boosheid (woede)': (-1, 0),\n",
    "    'Anticipatie': (-0.707, 0.707),\n",
    "    'Walging': (-0.707, -0.707),\n",
    "    'Vreugde': (0, 1),\n",
    "    'Verdriet': (0, -1),\n",
    "    'Verassing': (0.707, -0.707),\n",
    "    'Geen emoti': (100, 100),\n",
    "    'Geen Emoti': (100, 100),\n",
    "    'Niet Bruikbaar': (-100, -100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_annotations = pd.read_pickle('numerical_agreement_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_distance_function(label1, label2):\n",
    "    '''\n",
    "        This function takes the euclidean distance between two numerical emotion labels.\n",
    "        To this end, it first converts the frozenset input to a list.\n",
    "        Subsequently it considers special cases (no emotion & not usable).\n",
    "        Lastly, it normalizes the output to be a number between 0 and 1 as this is required for NLTK agreement \n",
    "        metrics. It does this by dividing the output by two as the maximum distance is equal to 2.\n",
    "    '''\n",
    "    label1 = list(label1)\n",
    "    label2 = list(label2)\n",
    "    assert len(label1) > 0 and len(label2) > 0, \"Labels have length zero\"\n",
    "    \n",
    "    # If a label indicates 'no emotion' AND 'not usable' change it to 'not usable'\n",
    "    if label1 == [(100, 100), (-100, -100)] or label1 == [(-100, -100), (100, 100)]:\n",
    "        label1 = [(-100, -100)]\n",
    "    elif label2 == [(100, 100), (-100, -100)] or label2 == [(-100, -100), (100, 100)]:\n",
    "        label2 = [(-100, -100)]\n",
    "    \n",
    "    # If both labels indicate 'no emotion' distance is zero (full agreement)\n",
    "    if label1 == [(100, 100)] and label2 == [(100, 100)]:\n",
    "        return 0\n",
    "    # If both labels indicate 'not usable' distance is zero (full agreement)\n",
    "    elif label1 == [(-100, -100)] and label2 == [(-100, -100)]:\n",
    "        return 0\n",
    "    # If one label indicates 'no emotion' and the other 'not usable', we argue that there is some agreement\n",
    "    elif (label1 == [(-100, -100)] and label2 == [(100, 100)]) or (label2 == [(-100, -100)] and label1 == [(100, 100)]):\n",
    "        return 0.5\n",
    "    # If one label indicates 'no emotion' or 'not usable' and the other indicates emotion, there is no agreement\n",
    "    # distance is therefore equal to one\n",
    "    elif (label1 == [(100, 100)] and label2 != [(100, 100)]) or (label1 == [(-100, -100)] and label2 != [(-100, -100)]):\n",
    "        return 1\n",
    "    # The same for label2\n",
    "    elif (label2 == [(100, 100)] and label1 != [(100, 100)]) or (label2 == [(-100, -100)] and label1 != [(-100, -100)]):\n",
    "        return 1\n",
    "    # If one label contains 'no emotion' and the other does not, there is no agreement (distance 1)\n",
    "    elif ((100, 100) in label1 and (100, 100) not in label2) or ((100, 100) in label2 and (100, 100) not in label1):\n",
    "        return 1\n",
    "    # Special case: When someone used 'not usable' together with an emotion, and another did not, we calculate \n",
    "    # the distance between the emotion labels, and we add 1 for the 'not usable' label (normalised it becomes 1/2)\n",
    "    elif ((-100, -100) in label1 and (-100, -100) not in label2 and len(label1) > 1):\n",
    "        label1.remove((-100, -100))\n",
    "        if len(label1) > 1 or len(label2)  >1:\n",
    "            distances = [distance.euclidean(one, two) for one in label1 for two in label2]\n",
    "            return ((sum(distances) / len(distances)) / 2) + 0.5\n",
    "        elif len(label1) == 1 and len(label2) == 1:\n",
    "            return (distance.euclidean(label1, label2) / 2) + 0.5\n",
    "    # The same thing, but now for label2    \n",
    "    elif ((-100, -100) in label2 and (-100, -100) not in label1 and len(label2) > 0):\n",
    "        label2.remove((-100, -100))\n",
    "        if len(label1) > 1 or len(label2)  >1:\n",
    "            distances = [distance.euclidean(one, two) for one in label1 for two in label2]\n",
    "            return ((sum(distances) / len(distances)) / 2) + 0.5\n",
    "        elif len(label1) == 1 and len(label2) == 1:\n",
    "            return (distance.euclidean(label1, label2) / 2) + 0.5\n",
    "    # The straightforward case (i.e. one emotion label per person)\n",
    "    elif len(label1) == 1 and len(label2) == 1:\n",
    "        return distance.euclidean(label1, label2) / 2\n",
    "    # The case for multiple labels\n",
    "    else:\n",
    "        distances = [distance.euclidean(one, two) for one in label1 for two in label2]\n",
    "        return (sum(distances) / len(distances)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset(s, min_size):\n",
    "    powerset = []\n",
    "    x = len(s)\n",
    "    for i in range(1 << x):\n",
    "        powerset.append([s[j] for j in range(x) if (i & (1 << j))])\n",
    "        \n",
    "    powerset = [i for i in powerset if len(i) > min_size]\n",
    "        \n",
    "    return powerset\n",
    "\n",
    "def agreement_measures(annotator_dataframe, annotators):\n",
    "    \n",
    "    annotations = triples_list(annotator_dataframe, annotators)\n",
    "        \n",
    "    annotation_task = AnnotationTask(data = annotations, distance = masi_distance)\n",
    "    annotation_task_jaccard = AnnotationTask(data=annotations, distance=jaccard_distance)\n",
    "    annotation_task_custom = AnnotationTask(data = annotations, distance=custom_distance_function)\n",
    "    \n",
    "    # Single-metric evaluation of the annotation task\n",
    "    alpha_masi = annotation_task.alpha()\n",
    "    alpha_jaccard = annotation_task_jaccard.alpha()\n",
    "    alpha_custom = annotation_task_custom.alpha()\n",
    "    multi_kappa = annotation_task_custom.multi_kappa()\n",
    "    multi_pi = annotation_task_custom.pi()\n",
    "    \n",
    "    return alpha_masi, alpha_jaccard, alpha_custom, multi_kappa, multi_pi\n",
    "    \n",
    "def find_best_team(df, annotators):\n",
    "    if len(annotators)  <= 1:\n",
    "        return {}\n",
    "    \n",
    "    metrics = agreement_measures(df, annotators)\n",
    "    return {\n",
    "        'team': annotators,\n",
    "        'alpha_masi': metrics[0],\n",
    "        'alpha_jaccard': metrics[1],\n",
    "        'alpha_custom': metrics[2],\n",
    "        'multi_kappa': metrics[3],\n",
    "        'multi_pi': metrics[4]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 536 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1536 tasks      | elapsed:   29.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2936 tasks      | elapsed:   58.2s\n",
      "[Parallel(n_jobs=-1)]: Done 4058 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4608 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5258 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6008 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 6858 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 7808 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8858 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 10008 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 11258 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 12608 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 14058 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 15608 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 17258 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 19008 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 20858 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 22808 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=-1)]: Done 24858 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=-1)]: Done 27008 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=-1)]: Done 29258 tasks      | elapsed: 15.6min\n",
      "[Parallel(n_jobs=-1)]: Done 31608 tasks      | elapsed: 17.3min\n",
      "[Parallel(n_jobs=-1)]: Done 35388 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=-1)]: Done 40488 tasks      | elapsed: 22.1min\n",
      "[Parallel(n_jobs=-1)]: Done 45788 tasks      | elapsed: 25.5min\n",
      "[Parallel(n_jobs=-1)]: Done 49136 tasks      | elapsed: 28.1min\n",
      "[Parallel(n_jobs=-1)]: Done 51986 tasks      | elapsed: 29.6min\n",
      "[Parallel(n_jobs=-1)]: Done 54936 tasks      | elapsed: 31.5min\n",
      "[Parallel(n_jobs=-1)]: Done 57986 tasks      | elapsed: 33.6min\n",
      "[Parallel(n_jobs=-1)]: Done 61136 tasks      | elapsed: 36.0min\n",
      "[Parallel(n_jobs=-1)]: Done 64386 tasks      | elapsed: 38.8min\n",
      "[Parallel(n_jobs=-1)]: Done 65399 out of 65399 | elapsed: 39.9min finished\n"
     ]
    }
   ],
   "source": [
    "teams =  Parallel(n_jobs = -1, verbose = 1)(delayed(find_best_team)(result[0], team) for team in powerset(annotators,2))\n",
    "\n",
    "df_results = pd.DataFrame(teams).dropna(subset = ['team']).set_index('team')\n",
    "\n",
    "df_results = df_results.sort_values(by = 'alpha_custom', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_pickle('results_totaal.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_permutations(df, annotators):\n",
    "    perm = list(permutations(annotators))\n",
    "    metrics = []\n",
    "    for team in perm:\n",
    "        agreement = agreement_measures(df, team)\n",
    "        metrics.append({\n",
    "            'alpha_masi': agreement[0],\n",
    "            'alpha_jaccard': agreement[1],\n",
    "            'alpha_custom': agreement[2],\n",
    "            'multi_kappa': agreement[3],\n",
    "            'multi_pi': agreement[4]\n",
    "        }) \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 25.7min finished\n"
     ]
    }
   ],
   "source": [
    "results_permutation =  Parallel(n_jobs = -1, verbose = 1)(delayed(research_permutations)(result[0], team) for team in random.sample(list(combinations(annotators, 6)), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_permutation = [item for sublist in results_permutation for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_permutation_experiment.pkl', 'wb') as f:\n",
    "    pickle.dump(final_results_permutation, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
