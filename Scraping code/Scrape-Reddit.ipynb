{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import pprint\n",
    "import emoji\n",
    "import regex\n",
    "from textblob import TextBlob\n",
    "from textblob_nl import PatternTagger, PatternAnalyzer\n",
    "from prawcore.exceptions import Forbidden\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.1.0 of praw is outdated. Version 7.2.0 was released Wednesday February 24, 2021.\n"
     ]
    }
   ],
   "source": [
    "# Initialise the API python wrapper\n",
    "reddit = praw.Reddit(client_id='HeUHguuE1BBilA', \n",
    "                     client_secret='Qc0xkQ1_Z4calpnL1Vx9k6i-2wCQTg', \n",
    "                     user_agent='Jasper Nieuwdorp',\n",
    "                    username = 'jaspernieuwdorp',\n",
    "                    password = 't6AD97359K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These sub-reddits are scraped (see also: Appendix)\n",
    "nederlandse_subreddits = [\n",
    "    'cirkeltrek', 'Geschiedenis', 'papgrappen', 'RMTK', 'D66', 'Politiek'\n",
    "]\n",
    "\n",
    "nederlandse_subreddits_ronde2 = [\n",
    "    'coronanetherlands', 'eindhoven', 'roermond', 'tokkiefeesboek', 'nederlands', 'VeganNL'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_count(text):\n",
    "    '''\n",
    "    The idea was to extract also emojis from the posts and comments. \n",
    "    I thought it might be interesting to do something with that as well, but time prohibited me.\n",
    "    '''\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reddit(subreddits, limit):\n",
    "    '''\n",
    "    Given a list of subreddits, and given a limit of the amounts of posts,\n",
    "    scrape the posts of the subreddit, and its comments.\n",
    "    '''\n",
    "    dataframe = []\n",
    "\n",
    "    for sub in subreddits:\n",
    "        try:\n",
    "            hot_posts = reddit.subreddit(sub).hot(limit = limit)\n",
    "            for post in hot_posts:\n",
    "                available_attributes = vars(post)\n",
    "                timestamp = post.created_utc if 'created_utc' in available_attributes else None\n",
    "                title = post.title if 'title' in available_attributes else None\n",
    "                text = post.selftext if 'selftext' in available_attributes else None\n",
    "\n",
    "                if text is not None and len(text) > 0:\n",
    "                    # Find the emojis in a post\n",
    "                    emoji_lst = split_count(text)\n",
    "                    if len(emoji_lst) > 0:\n",
    "                        emojis = [emoji for emoji in emoji_lst]\n",
    "                    else:\n",
    "                        emojis = []\n",
    "                    # Calculate the TextBlob sentiment scores\n",
    "                    blob = TextBlob(text, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "                    text_sentiment = blob.sentiment\n",
    "                else:\n",
    "                    emojis = []\n",
    "                    text_sentiment = None\n",
    "\n",
    "                d = {\n",
    "                    'timestamp':timestamp,\n",
    "                    'title':title,\n",
    "                    'text':text.strip().replace('\\n', ' '),\n",
    "                    'sentiment':text_sentiment,\n",
    "                    'emojis': emojis,\n",
    "                    'ups': post.ups,\n",
    "                    'downs': post.downs\n",
    "                }\n",
    "\n",
    "                dataframe.append(d)\n",
    "\n",
    "                for comment in post.comments:\n",
    "                    post.comments.replace_more(limit=None)\n",
    "                    for comment in post.comments.list():\n",
    "                        available_attributes = vars(comment)\n",
    "                        timestamp = comment.created_utc if 'created_utc' in available_attributes else None\n",
    "                        title = None\n",
    "                        text = comment.body\n",
    "\n",
    "                        if text is not None and len(text) > 0:\n",
    "                            # Find the emojis in a post\n",
    "                            emoji_lst = split_count(text)\n",
    "                            if len(emoji_lst) > 0:\n",
    "                                emojis = [emoji for emoji in emoji_lst]\n",
    "\n",
    "                                # Calculate the TextBlob sentiment scores\n",
    "                                blob = TextBlob(text, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "                                text_sentiment = blob.sentiment\n",
    "                            else:\n",
    "                                emojis = []\n",
    "                                text_sentiment = None\n",
    "                        d = {\n",
    "                            'timestamp':timestamp,\n",
    "                            'title':title,\n",
    "                            'text':text.strip().replace('\\n', ' '),\n",
    "                            'sentiment':text_sentiment,\n",
    "                            'emojis': emojis,\n",
    "                            'ups': comment.ups,\n",
    "                            'downs': comment.downs\n",
    "                        }\n",
    "\n",
    "                        dataframe.append(d)\n",
    "        except Forbidden:\n",
    "            print(Forbidden)\n",
    "            print(f\"We're not allowed to acces subreddit: {sub}\")\n",
    "\n",
    "    return pd.DataFrame(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_reddit(nederlandse_subreddits_ronde2, 10000).to_csv('reddit-results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labelling",
   "language": "python",
   "name": "labelling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
